<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SRL Labs | Surgical Robot Learning</title>
    <meta name="description" content="Building the first surgical robotics VLA foundation model. We are automating surgery, end-to-end.">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Header -->
    <header class="header">
        <div class="container">
            <div class="nav-wrapper">
                <div class="logo">SRL_LABS</div>
                <nav class="nav">
                    <a href="#vision">vision</a>
                    <a href="#approach">approach</a>
                    <a href="#contact">contact</a>
                </nav>
            </div>
        </div>
    </header>

    <!-- Hero Section -->
    <section class="hero">
        <div class="container">
            <div class="hero-grid">
                <div class="hero-left">
                    <h1 class="hero-title">
                        Surgical Robot Learning Labs<span class="accent">.</span>
                    </h1>
                    <div class="divider">────────────────────────────────────────</div>
                    <div class="tagline">
                        we are automating surgery. end-to-end.
                    </div>

                    <!-- Building Block -->
                    <div class="feature-block">
                        <div class="feature-title">BUILDING THE FUTURE</div>
                        <div class="feature-list">
                            <div>&gt; RL environments for surgery</div>
                            <div>&gt; VLA foundation models</div>
                            <div>&gt; relative-motion policies</div>
                            <div>&gt; spatial reasoning traces</div>
                            <div>&gt; steerable autonomy</div>
                        </div>
                    </div>

                    <!-- PhD Research & Partnerships -->
                    <div class="research-note">
                        <div class="note-text">
                            PhD students at Oxford Robotics Institute<br/>
                            collaborating with Stanford, Johns Hopkins, and da Vinci Research Kit
                        </div>
                    </div>
                </div>

                <!-- Right side with CLI Animation -->
                <div class="hero-right">
                    <!-- CLI Animation -->
                    <div class="cli-container">
                        <div class="cli-line">
                            <span id="cli-text"></span><span class="cursor">_</span>
                        </div>

                        <!-- Building Status -->
                        <div class="building-status">
                            <div class="status-line">
                                <span class="status-building">[BUILDING]</span>
                                <span>tissue_handling_env.py</span>
                            </div>
                            <div class="status-line">
                                <span class="status-building">[BUILDING]</span>
                                <span>needle_passing_policy.pt</span>
                            </div>
                            <div class="status-line">
                                <span class="status-building">[BUILDING]</span>
                                <span>knot_tying_vla.safetensors</span>
                            </div>
                            <div class="status-line">
                                <span class="status-building">[BUILDING]</span>
                                <span>spatial_reasoning_module.rs</span>
                            </div>
                        </div>

                        <!-- Current Focus -->
                        <div class="current-focus">
                            <div class="focus-title">current focus:</div>
                            <div class="focus-items">
                                <div>• training on 10M+ da Vinci surgical videos</div>
                                <div>• handling 15% kinematic noise tolerance</div>
                                <div>• achieving <5ms latency for haptic feedback</div>
                                <div>• building sim2real transfer with 85% success</div>
                                <div>• integrating with dVRK teleoperation API</div>
                            </div>
                        </div>
                    </div>

                    <!-- Vision Statement -->
                    <div class="vision-box">
                        <div class="vision-text">
                            turning da Vinci robots from passive instruments<br/>
                            into adaptive partners in the operating room
                        </div>
                    </div>

                    <!-- Technical Approach -->
                    <div class="tech-note">
                        <code>architecture: ViT-22B backbone + diffusion decoder + MPC safety layer</code>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Vision Section -->
    <section id="vision" class="vision">
        <div class="container">
            <h2 class="section-title">The Problem</h2>
            <div class="problem-text">
                <p>Surgeons operate for hours under fatigue.</p>
                <p>Today's da Vinci robots are passive tools with zero autonomy.</p>
                <p>Surgical errors rise sharply after long procedures.</p>
            </div>

            <div class="divider-section">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</div>

            <h2 class="section-title">Our Vision</h2>
            <div class="vision-statement">
                <p>
                    We are building a surgical robotics Vision-Language-Action (VLA) foundation model
                    that learns from millions of surgical demonstrations to perform fundamental surgical tasks:
                    tissue handling, needle passing, knot tying, and suture running.
                </p>
                <p>
                    Our model processes RGB-D surgical video at 30Hz, outputting 6-DOF end-effector
                    trajectories with sub-millimeter precision. We handle the unique challenges of
                    surgical data: noisy kinematics, variable tissue properties, and safety constraints.
                </p>
                <p class="emphasis">
                    The first scalable, interpretable, and steerable surgical foundation model—
                    turning passive surgical robots into adaptive partners.
                </p>
            </div>
        </div>
    </section>

    <!-- Approach Section -->
    <section id="approach" class="approach">
        <div class="container">
            <h2 class="section-title">Our Approach</h2>

            <div class="approach-grid">
                <div class="approach-item">
                    <div class="approach-title">relative-motion policies</div>
                    <div class="approach-desc">
                        Training directly on noisy da Vinci kinematic data using relative action spaces.
                        Our approach handles miscalibration and drift, learning from 10M+ real surgical
                        procedures without requiring perfect robot calibration.
                    </div>
                </div>

                <div class="approach-item">
                    <div class="approach-title">spatial reasoning traces</div>
                    <div class="approach-desc">
                        Generating interpretable 3D trajectories before execution. Surgeons can visualize,
                        edit, and approve planned motions. Built on diffusion models that output depth maps,
                        segmentation masks, and action tokens.
                    </div>
                </div>

                <div class="approach-item">
                    <div class="approach-title">steerable autonomy</div>
                    <div class="approach-desc">
                        Real-time intervention through pointing, sketching, or direct control. Uses
                        stochastic sampling with safety constraints to ensure corrections stay within
                        surgical boundaries. Zero-shot adaptation to surgeon preferences.
                    </div>
                </div>
            </div>

            <div class="tech-stack">
                <div class="stack-label">technical stack:</div>
                <div class="stack-items">
                    <span>PyTorch 2.0</span>
                    <span>•</span>
                    <span>IsaacSim</span>
                    <span>•</span>
                    <span>ROS2 Humble</span>
                    <span>•</span>
                    <span>CUDA 12.1</span>
                    <span>•</span>
                    <span>Transformers</span>
                    <span>•</span>
                    <span>dVRK</span>
                </div>
                <div class="stack-specs">
                    Model: 7B parameters • Inference: 30Hz on RTX 4090 • Training: 256 A100 GPUs • Dataset: 10M+ surgical videos
                </div>
            </div>
        </div>
    </section>

    <!-- Contact Section -->
    <section id="contact" class="contact">
        <div class="container">
            <div class="contact-content">
                <h2 class="section-title">Get in Touch</h2>
                <p class="contact-text">
                    We're building the future of surgical autonomy.<br/>
                    Currently in stealth at Oxford Robotics Institute.
                </p>

                <div class="partnerships">
                    <div class="partnership-label">research partners:</div>
                    <div class="partnership-list">
                        Stanford AI Lab • Johns Hopkins LCSR • da Vinci Research Kit Consortium
                    </div>
                </div>

                <div class="contact-details">
                    <div class="contact-email">research@surgicalRL.io</div>
                </div>

                <div class="location">
                    <div>Oxford, UK • Stanford, CA • Baltimore, MD</div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div>© 2025 SRL Labs</div>
                <div>building the future of surgery</div>
            </div>
        </div>
    </footer>

    <!-- Background Grid -->
    <div class="grid-background"></div>

    <!-- CLI Animation Script -->
    <script>
        const commands = [
            '> building: tissue_handling_env.py',
            '> training: needle_passing_vla.pt',
            '> compiling: spatial_reasoning.rs',
            '> testing: steerable_policy_v0.1'
        ];

        let currentCommand = 0;
        let currentChar = 0;
        let isDeleting = false;
        let isPaused = false;

        function typeCommand() {
            const element = document.getElementById('cli-text');
            const command = commands[currentCommand];

            if (!isDeleting && currentChar <= command.length) {
                element.textContent = command.substring(0, currentChar);
                currentChar++;
                setTimeout(typeCommand, 50);
            } else if (!isDeleting && !isPaused) {
                isPaused = true;
                setTimeout(() => {
                    isPaused = false;
                    isDeleting = true;
                    typeCommand();
                }, 2000);
            } else if (isDeleting && currentChar > 0) {
                element.textContent = command.substring(0, currentChar - 1);
                currentChar--;
                setTimeout(typeCommand, 30);
            } else if (isDeleting) {
                isDeleting = false;
                currentCommand = (currentCommand + 1) % commands.length;
                setTimeout(typeCommand, 500);
            }
        }

        // Start animation
        typeCommand();
    </script>
</body>
</html>